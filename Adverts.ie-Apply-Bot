
from selenium import webdriver
from urllib.request import Request, urlopen
from bs4 import BeautifulSoup as soup
import time





pages = (1,2,3) 

for page in pages:
	url = 'https://www.adverts.ie/jobs/3/county-Galway/page-{}'.format(page)

	req = Request(url , headers={'User-Agent': 'Mozilla/5.0'})

	webpage = urlopen(req).read()
	page_soup = soup(webpage, "html.parser")



	

	links = page_soup.findAll("a", {"class": "main-image"})


	def Scrape_function():
		
		for link in links:

		    link = str("https://www.adverts.ie" + link['href'])

		    link_list = []

		    link_list.append(link)

		    return link_list


	
link_urls = Scrape_function()


keys = {
	"name": "",
	"email": "",
	"password": "",
	"cover_letter": ""
}

	
def applyFunction():

	driver = webdriver.Chrome('./chromedriver')

	for url in link_urls:
		driver.get(url)

		driver.find_element_by_xpath('//*[@id="name"]').click()
		driver.find_element_by_xpath('//*[@id="name"]').send_keys(keys["name"])
		driver.find_element_by_xpath('//*[@id="email"]').click()
		driver.find_element_by_xpath('//*[@id="email"]').send_keys(keys["email"])
		driver.find_element_by_xpath('//*[@id="message"]').click()
		driver.find_element_by_xpath('//*[@id="message"]').send_keys(keys["cover_letter"])
		driver.find_element_by_xpath('//*[@id="cv"]').send_keys("C:/Users/Me/Desktop/PYTHON BOT/cv.docx")
		driver.find_element_by_xpath('//*[@id="question-form"]/fieldset/div/div[5]/input').click()


applyFunction()




